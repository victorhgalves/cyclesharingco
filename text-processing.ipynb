{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Challenge - NLP ",
      "provenance": [],
      "collapsed_sections": [
        "KzjVBRtejGdQ",
        "5N_1gYf3jJu3"
      ],
      "authorship_tag": "ABX9TyNyqEp0hQwP6HnkTXSRsJH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorhgalves/cyclesharingco/blob/main/text-processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzjVBRtejGdQ"
      },
      "source": [
        "#Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Odp7hRg-iz"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7x4-VGUf3T6"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "from string import punctuation\n",
        "from wordcloud import WordCloud\n",
        "from nltk import tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N_1gYf3jJu3"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-M_4j7pjNCH"
      },
      "source": [
        "df = pd.read_csv('....')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_8pPvY_jQD1"
      },
      "source": [
        "#Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8gDXC2nCgX"
      },
      "source": [
        "def separete_sentences_whitespace(data):\n",
        "    '''Function responsible for separating phrases by whitespace'''\n",
        "    stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "    token_space =  tokenize.WhitespaceTokenizer()\n",
        "    processed_phrase = list()\n",
        "    for op in data:\n",
        "        new_phrase = list()\n",
        "        word_text = token_space.tokenize(op)\n",
        "        for word in word_text:\n",
        "          if word not in stop_words:\n",
        "            new_phrase.append(word)\n",
        "        processed_phrase.append(' '.join(new_phrase))\n",
        "    return processed_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sMfLU3CjTmk"
      },
      "source": [
        "def remove_ponctuation(data):\n",
        "    '''Function responsible for remove all ponctuation'''\n",
        "    token_punctuation = tokenize.WordPunctTokenizer()\n",
        "    list_punctuation = list()\n",
        "    for punct in punctuation:\n",
        "        list_punctuation.append(punct)\n",
        "\n",
        "    punct_stopwords = list_punctuation + stop_words\n",
        "    processed_phrase = list()\n",
        "    for op in data:\n",
        "        new_phrase = list()\n",
        "        text_word = token_punctuation.tokenize(op)\n",
        "        for word in text_word:\n",
        "            if word not in punct_stopwords:\n",
        "                new_phrase.append(word)\n",
        "        processed_phrase.append(' '.join(new_phrase))\n",
        "    return processed_phrase, punct_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jv92yK1kOvX"
      },
      "source": [
        "def remove_accent(data, punct_stopwords):\n",
        "    '''Function responsible for remove all accent'''\n",
        "    whiout_accent = [unidecode.unidecode(x) for x in data]\n",
        "    stopwords_whiout_accent = [unidecode.unidecode(x) for x in punct_stopwords]\n",
        "    data = whiout_accent\n",
        "    for op in data:\n",
        "      new_phrase = list()\n",
        "      text_word = token_punctuation.tokenize(op)\n",
        "      for word in text_word:\n",
        "          if word not in stopwords_whiout_accent:\n",
        "              new_phrase.append(word)\n",
        "      processed_phrase.append(' '.join(new_phrase))\n",
        "    return processed_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB5I96QXlKPO"
      },
      "source": [
        "def transform_text_lower(data):\n",
        "    '''Function responsible for transform all text in lower case'''\n",
        "    processed_phrase = list()\n",
        "    for op in data:\n",
        "        new_phrase = list()\n",
        "        op = op.lower()\n",
        "        text_word = token_punctuation.tokenize(op)\n",
        "        for word in text_word:\n",
        "            if word not in stopwords_whiout_accent:\n",
        "                new_phrase.append(word)\n",
        "        processed_phrase.append(' '.join(new_phrase))\n",
        "    return processed_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB_G1G9QljhC"
      },
      "source": [
        "def set_stemmer(data):\n",
        "    '''Function responsible for aplicate stemmer in text'''\n",
        "    stemmer = nltk.RSLPStemmer()\n",
        "    processed_phrase = list()\n",
        "    for op in data:\n",
        "        new_phrase = list()\n",
        "        op = op.lower()\n",
        "        text_word = token_punctuation.tokenize(op)\n",
        "        for word in text_word:\n",
        "            if word not in stopwords_whiout_accent:\n",
        "                new_phrase.append(stemmer.stem(word))\n",
        "        processed_phrase.append(' '.join(new_phrase))\n",
        "    return processed_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JigajIYmUoY"
      },
      "source": [
        "def vectorize_transform(data):\n",
        "    '''Function responsible for aplicate TfidfVectorize'''\n",
        "    vectorizer = TfidfVectorizer(lowercase = False, ngram_range = (1,2))\n",
        "    bag_of_words = vectorizer.fit_transform(data)\n",
        "    return bag_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX_Kpa8NK6F1"
      },
      "source": [
        "def lang_detect(data):\n",
        "   '''Function to return phrase language using yours stop words'''\n",
        "    words = set(nltk.wordpunct_tokenize(text.lower()))\n",
        "    lang = max(((lang, len(words & stopwords)) for lang, stopwords in dicionario_stopwords.items()), key = lambda x: x[1])[0]\n",
        "    return lang"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}